<div align="center">

  <h1>
    <img src="public/favicon.png" alt="OpenGPT" width="64" height="64" align="absmiddle"> OpenGPT
  </h1>
  <p style="margin-top: 4px;">Experiment with open-source AI models</p>

[![Next.js](https://img.shields.io/badge/Next.js-15.4.6-black?style=flat-square&logo=next.js)](https://nextjs.org/) [![React](https://img.shields.io/badge/React-19.1.0-61DAFB?style=flat-square&logo=react)](https://reactjs.org/) [![TypeScript](https://img.shields.io/badge/TypeScript-5.x-3178C6?style=flat-square&logo=typescript)](https://www.typescriptlang.org/) [![Cloudflare Workers](https://img.shields.io/badge/Cloudflare-Workers-F38020?style=flat-square&logo=cloudflare)](https://workers.cloudflare.com/) [![AI SDK](https://img.shields.io/badge/AI_SDK-5.0.34-FF6154?style=flat-square&logo=vercel)](https://sdk.vercel.ai/) [![Tailwind CSS](https://img.shields.io/badge/Tailwind-4.x-06B6D4?style=flat-square&logo=tailwindcss)](https://tailwindcss.com/) [![License](https://img.shields.io/badge/License-MIT-green?style=flat-square)](LICENSE)

<br />

A modern AI playground that combines the **development experience of Next.js** with the **performance of Cloudflare Workers**. Experiment with 50+ open-source AI models, including GPT-OSS, Leonardo, Llama, Qwen, Gemini, DeepSeek, and more. Features text-to-speech with multiple voices and real-time speech-to-text transcription.

</div>

<div align="center">

<video width="360" height="640" controls>
  <source src="OpenGPT-Demo.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>

</div>

## **Why OpenGPT?**

<div align="center">

### üèÜ **Best of Both Worlds**

**Development Experience** üíª + **Deployment Performance** ‚ö°

</div>

OpenGPT leverages three core technologies to deliver an exceptional AI development experience:

### üîß **Core Technologies**

| Technology                   | What it brings                                   | Why it matters                                                              |
| ---------------------------- | ------------------------------------------------ | --------------------------------------------------------------------------- |
| **üîó OpenNext**              | Seamless Next.js ‚Üí Cloudflare Workers deployment | Deploy Next.js apps globally with the most affordable edge compute offering |
| **ü§ñ AI SDK v5**             | Universal AI framework with streaming support    | Connect to any AI provider with type-safe, streaming APIs                   |
| **‚òÅÔ∏è Cloudflare Workers AI** | Global AI inference                              | Sub-100ms latency worldwide with 50+ open-source models                     |

## üåü **Features**

### üí¨ **Multi-Modal AI Interface**

- **Chat Mode**: Conversational AI with 50+ text generation models
- **Image Mode**: High-quality image generation with 5+ image models
- **Text-to-Speech (TTS)**: Voice synthesis with multiple speaker options
- **Speech-to-Text (STT)**: Real-time audio transcription with visual feedback
- **Seamless Switching**: Toggle between modes without losing context

### üß† **Advanced Reasoning Support**

- **Thinking Process Visualization**: See how AI models reason through problems
- **Collapsible Reasoning**: Clean UI that shows/hides reasoning on demand
- **Universal Compatibility**: Works with any AI model that supports reasoning tokens

### üé® **Modern User Experience**

- **AI Elements UI**: Professional, accessible components built using [AI Elements](https://ai-sdk.dev/elements/overview)
- **Responsive Design**: Mobile-first with smooth interactions
- **Real-time Streaming**: See responses as they're generated

### üîß **Developer Experience**

- **Type Safety**: Full TypeScript with Cloudflare bindings
- **One-Command Deploy**: `pnpm deploy` to Cloudflare Workers globally

## üöÄ **Getting Started**

### Installation

```bash

# Clone the repository
git clone https://github.com/devhims/opengpt.git
cd opengpt

# Install dependencies
pnpm install

# Start development server
pnpm dev
```

Visit [http://localhost:3000](http://localhost:3000) to see OpenGPT in action! üéâ

### Environment Setup

1. **Create `.dev.vars`** for local development:

```bash
# .dev.vars (not committed to git)
NEXTJS_ENV=development
```

2. **For production secrets**:

```bash
wrangler secret put NEXTJS_ENV
```

## üõ†Ô∏è **Available Scripts**

| Command           | Description                                     |
| ----------------- | ----------------------------------------------- |
| `pnpm dev`        | Start development server with Turbopack         |
| `pnpm build`      | Build the Next.js application                   |
| `pnpm preview`    | Preview the Cloudflare Workers build locally    |
| `pnpm deploy`     | Build and deploy to Cloudflare Workers globally |
| `pnpm lint`       | Run ESLint with TypeScript rules                |
| `pnpm format`     | Format code with Prettier                       |
| `pnpm cf-typegen` | Generate Cloudflare binding types               |

## ü§ñ **Supported AI Models**

### Text Generation (50+ Models)

- **GPT-OSS**: OpenAI-compatible 20B and 120B variants
- **Meta Llama**: 4 Scout 17B, 3.3 70B, 3.1 family (6 variants), 3.2 family (3 variants), 3.0 family (3 variants)
- **Google Gemma**: 3 12B IT, 7B IT, and LoRA variants (4 total)
- **Mistral**: Small 3.1 24B, 7B v0.1/v0.2 variants (5 total)
- **Qwen**: QWQ 32B, 2.5 Coder 32B, and 1.5 family variants (6 total)
- **DeepSeek**: R1 Distill Qwen 32B, Math 7B, Coder variants (4 total)
- **Other Models**: Falcon, Phi-2, TinyLlama, SQLCoder, and 10+ specialized models

### Image Generation (5+ Models)

- **Black Forest Labs**: FLUX-1-Schnell (fast, high-quality text-to-image)
- **Leonardo AI**: Lucid Origin and Phoenix 1.0
- **Stability AI**: Stable Diffusion XL Base 1.0
- **ByteDance**: Stable Diffusion XL Lightning (ultra-fast generation)

### Speech & Audio (3+ Models)

- **Text-to-Speech (TTS)**:
  - **Deepgram Aura-1**: 12+ expressive voices (Luna, Athena, Zeus, Angus, etc.)
  - **MyShell.ai MeloTTS**: Multi-language support (EN, ES, FR, ZH, JP, KR) with regional accents
- **Speech-to-Text (STT)**:
  - **Deepgram Nova-3**: High-accuracy real-time transcription with punctuation

## üèóÔ∏è **Architecture**

OpenGPT showcases a modern, production-ready architecture with comprehensive request handling:

```mermaid
flowchart TD
    User[üë§ User] --> UI[üé® Next.js Frontend]
    UI --> ModeToggle{Mode Selection}

    ModeToggle -->|üí¨ Chat| ChatPath[Chat Request Path]
    ModeToggle -->|üñºÔ∏è Image| ImagePath[Image Request Path]
    ModeToggle -->|üó£Ô∏è Speech| SpeechPath[Speech Request Path]

    ChatPath --> ChatAPI[üì° /api/chat]
    ImagePath --> ImageAPI[üì° /api/image]
    SpeechPath --> SpeechAPI[üì° /api/speech-to-text | /api/text-to-speech]

    ChatAPI --> RateLimit1[üö´ Rate Limiter]
    ImageAPI --> RateLimit2[üö´ Rate Limiter]
    SpeechAPI --> RateLimit3[üö´ Rate Limiter]

    RateLimit1 --> RateCheck1{Rate OK?}
    RateLimit2 --> RateCheck2{Rate OK?}
    RateLimit3 --> RateCheck3{Rate OK?}

    RateCheck1 -->|‚ùå| RateError1[429 Error]
    RateCheck1 -->|‚úÖ| ChatProcessing[ü§ñ Chat Processing]

    RateCheck2 -->|‚ùå| RateError2[429 Error]
    RateCheck2 -->|‚úÖ| ImageProcessing[üé® Image Processing]

    RateCheck3 -->|‚ùå| RateError3[429 Error]
    RateCheck3 -->|‚úÖ| SpeechProcessing[üó£Ô∏è Speech Processing]

    ChatProcessing --> ModelType{Model Type}
    ModelType -->|Standard| AISDKPath[üîß AI SDK v5 + workers-ai-provider]
    ModelType -->|GPT-OSS| DirectPath[üéØ Direct env.AI.run]

    ImageProcessing --> ImageAI[üé® Direct env.AI.run]
    SpeechProcessing --> SpeechAI[üó£Ô∏è Direct env.AI.run]

    AISDKPath --> WorkersAI1[‚òÅÔ∏è Cloudflare Workers AI]
    DirectPath --> WorkersAI2[‚òÅÔ∏è Cloudflare Workers AI]
    ImageAI --> WorkersAI3[‚òÅÔ∏è Cloudflare Workers AI]
    SpeechAI --> WorkersAI4[‚òÅÔ∏è Cloudflare Workers AI]

    WorkersAI1 --> Streaming[üåä Real-time Streaming]
    WorkersAI2 --> Batch[üìã Batch Processing + Emulated Stream]
    WorkersAI3 --> ImageResponse[üì∏ Generated Image]
    WorkersAI4 --> SpeechResponse[üîä Audio/Text Response]

    Streaming --> ParseReasoning[üß† Parse Reasoning]
    Batch --> ParseReasoning

    ParseReasoning --> ChatSuccess[‚úÖ Chat Response]
    ImageResponse --> ImageSuccess[‚úÖ Image Response]
    SpeechResponse --> SpeechSuccess[‚úÖ Speech Response]

    RateError1 --> ErrorUI[üé® Error Display]
    RateError2 --> ErrorUI
    RateError3 --> ErrorUI

    ChatSuccess --> ResponseUI[üì• Response Display]
    ImageSuccess --> ResponseUI
    SpeechSuccess --> ResponseUI
```

## üåä **Request Flow Architecture**

Detailed end-to-end request processing from user interaction to AI generation:

```mermaid
flowchart TD
    Start[üë§ User Input] --> InputType{Input Type}

    %% Chat Path
    InputType -->|üí¨ Chat Message| ChatUI[üé® Chat UI Processing]
    ChatUI --> ChatValidate[‚úÖ Validate Message]
    ChatValidate --> ChatRequest[üì° POST /api/chat]

    ChatRequest --> ChatParse[üìã Parse Request Body]
    ChatParse --> ChatRateLimit["üö´ checkRateLimit req chat"]
    ChatRateLimit --> ChatRateCheck{Rate Limit OK?}

    ChatRateCheck -->|‚ùå No| ChatRateError[429: Rate Limit Exceeded]
    ChatRateCheck -->|‚úÖ Yes| ChatModelCheck{Model Type}

    %% Chat - Standard Models Path
    ChatModelCheck -->|Standard Models| ChatStandard["üîß processMessages"]
    ChatStandard --> ChatWorkersAI["ü§ñ createWorkersAI binding env.AI"]
    ChatWorkersAI --> ChatStreamText["üåä streamText model messages"]
    ChatStreamText --> ChatWorkers1[‚òÅÔ∏è Cloudflare Workers AI]
    ChatWorkers1 --> ChatStream[üì§ Real-time SSE Stream]

    %% Chat - GPT-OSS Models Path
    ChatModelCheck -->|GPT-OSS Models| ChatGPT["üéØ handleGptOssModel"]
    ChatGPT --> ChatDirectRun["üì° env.AI.run model input"]
    ChatDirectRun --> ChatWorkers2[‚òÅÔ∏è Cloudflare Workers AI]
    ChatWorkers2 --> ChatExtract["üìã extractGptOssResponse"]
    ChatExtract --> ChatEmulated["üåä createUIMessageStream"]

    %% Image Path
    InputType -->|üñºÔ∏è Image Prompt| ImageUI[üé® Image UI Processing]
    ImageUI --> ImageValidate[‚úÖ Validate Prompt & Params]
    ImageValidate --> ImageRequest[üì° POST /api/image]

    ImageRequest --> ImageParse[üìã Parse Request Body]
    ImageParse --> ImageRateLimit2["üö´ checkRateLimit req image"]
    ImageRateLimit2 --> ImageRateCheck{Rate Limit OK?}

    ImageRateCheck -->|‚ùå No| ImageRateError[429: Rate Limit Exceeded]
    ImageRateCheck -->|‚úÖ Yes| ImageOptimal["üîß generateOptimalPayload"]
    ImageOptimal --> ImageDirectRun2["üì° env.AI.run model payload"]
    ImageDirectRun2 --> ImageWorkers[‚òÅÔ∏è Cloudflare Workers AI]

    %% Speech Path
    InputType -->|üé§ Voice Input| SpeechUI[üé® Speech UI Processing]
    InputType -->|üìù Text Input| TTSUI[üé® TTS UI Processing]

    SpeechUI --> SpeechValidate[‚úÖ Validate Audio File]
    SpeechValidate --> SpeechRequest[üì° POST /api/speech-to-text]

    TTSUI --> TTSValidate[‚úÖ Validate Text & Voice]
    TTSValidate --> TTSRequest[üì° POST /api/text-to-speech]

    SpeechRequest --> SpeechParse[üìã Parse Audio Request]
    SpeechParse --> SpeechRateLimit["üö´ checkRateLimit req speech"]
    SpeechRateLimit --> SpeechRateCheck{Rate Limit OK?}

    TTSRequest --> TTSParse[üìã Parse TTS Request]
    TTSParse --> TTSRateLimit["üö´ checkRateLimit req tts"]
    TTSRateLimit --> TTSRateCheck{Rate Limit OK?}

    SpeechRateCheck -->|‚ùå No| SpeechRateError[429: Rate Limit Exceeded]
    SpeechRateCheck -->|‚úÖ Yes| SpeechDirectRun["üì° env.AI.run @cf/deepgram/nova-3"]
    SpeechDirectRun --> SpeechWorkers[‚òÅÔ∏è Cloudflare Workers AI]

    TTSRateCheck -->|‚ùå No| TTSRateError[429: Rate Limit Exceeded]
    TTSRateCheck -->|‚úÖ Yes| TTSDirectRun["üì° env.AI.run @cf/deepgram/aura-1 | @cf/myshell-ai/melotts"]
    TTSDirectRun --> TTSWorkers[‚òÅÔ∏è Cloudflare Workers AI]

    %% Image Response Processing
    ImageWorkers --> ImageFormat{Response Format}
    ImageFormat -->|Base64| ImageBase64[üìù Extract response.image]
    ImageFormat -->|Binary Stream| ImageBinary["üîÑ streamToBase64"]
    ImageBase64 --> ImageConvert[üî¢ Convert to Uint8Array]
    ImageBinary --> ImageConvert

    %% Speech Response Processing
    SpeechWorkers --> SpeechExtract[üìù Extract transcription.text]
    SpeechExtract --> SpeechSuccess[‚úÖ STT Response with Text]

    TTSWorkers --> TTSFormat{Response Format}
    TTSFormat -->|Base64 Audio| TTSAudio[üîä Extract response.audio]
    TTSAudio --> TTSConvert[üîä Convert to playable audio]
    TTSConvert --> TTSSuccess[‚úÖ TTS Audio Response]

    %% Success Responses
    ChatStream --> ChatReasoning[üß† Parse Reasoning Tokens]
    ChatEmulated --> ChatReasoning
    ChatReasoning --> ChatSuccess[‚úÖ Chat Response with Reasoning]

    ImageConvert --> ImageSuccess[‚úÖ Image Response with Metadata]
    SpeechSuccess --> SpeechFinal[‚úÖ STT Response]
    TTSSuccess --> TTSFinal[‚úÖ TTS Response]

    %% Error Handling
    ChatRateError --> ErrorDisplay[üé® Rate Limit Banner]
    ImageRateError --> ErrorDisplay
    SpeechRateError --> ErrorDisplay
    TTSRateError --> ErrorDisplay

    %% Final Display
    ChatSuccess --> FinalDisplay[üì± Frontend Display]
    ImageSuccess --> FinalDisplay
    SpeechFinal --> FinalDisplay
    TTSFinal --> FinalDisplay
    ErrorDisplay --> FinalDisplay

    FinalDisplay --> UserExperience[üë§ User Sees Result]
```

### Key Implementation Details

**Chat Route Processing:**

- **Standard Models**: Uses AI SDK v5 with `workers-ai-provider` wrapper for streaming
- **GPT-OSS Models**: Direct `env.AI.run` call with emulated streaming via `createUIMessageStream`
- **All models**: Connect to the same Cloudflare Workers AI backend

**Image Route Processing:**

- **All Image Models**: Direct `env.AI.run` call (no AI SDK wrapper needed)
- **Response Handling**: Supports both base64 and binary stream responses
- **Format Conversion**: Automatic conversion to both base64 and Uint8Array for frontend compatibility

**Speech Route Processing:**

- **Speech-to-Text**: Direct `env.AI.run` call with `@cf/deepgram/nova-3` model
- **Text-to-Speech**: Direct `env.AI.run` call with `@cf/deepgram/aura-1` or `@cf/myshell-ai/melotts` models
- **Audio Processing**: WebM/MP4 audio file handling with automatic format detection
- **Voice Options**: 12+ Aura-1 speakers, multi-language MeloTTS with regional accents

**Rate Limiting:**

- **Shared Infrastructure**: Both routes use the same `checkRateLimit` utility
- **Per-endpoint Limits**: Separate daily limits for chat (20), image (5), and speech (10) requests
- **Storage**: Hybrid Upstash Redis + Cloudflare KV fallback

### Key Architectural Decisions

- **üîó OpenNext**: Seamless Next.js to Cloudflare Workers deployment with global edge distribution
- **ü§ñ AI SDK v5**: Type-safe, streaming AI interactions with reasoning token support
- **üß† Reasoning Tokens**: Enhanced AI thinking process visualization with collapsible UI
- **üö´ Rate Limiting**: Hybrid Upstash Redis + Cloudflare KV approach with IP-based daily limits
- **‚ö° Multi-Modal Processing**: Separate optimized pathways for chat, image, and speech processing

### Request Processing Flow

1. **Frontend Validation**: Client-side input validation and optional rate limit pre-checking
2. **Rate Limiting**: IP-based daily limits (20 chat, 5 image, 10 speech requests) with Redis/KV storage
3. **Model Routing**: Smart routing between Standard Models (streaming) and GPT-OSS Models (batch)
4. **AI Processing**: Direct Cloudflare Workers AI integration with optimized parameters
5. **Response Handling**: Reasoning token parsing, format conversion, and UI display

## üöÄ **Deployment**

```bash
# Build and deploy in one command
pnpm deploy

# Or step by step
pnpm build
npx wrangler deploy
```

### Environment Variables

| Variable                   | Description                    |
| -------------------------- | ------------------------------ |
| `UPSTASH_REDIS_REST_URL`   | Upstash Redis URL (optional)   |
| `UPSTASH_REDIS_REST_TOKEN` | Upstash Redis token (optional) |

### Adding New Models

1. **Add model to constants**:

```typescript
// src/constants/index.ts
export const CLOUDFLARE_AI_MODELS = {
  textGeneration: [
    // Add your new model here
    '@cf/vendor/new-model',
    // ... existing models
  ] as const,
  imageGeneration: [
    // For image models
  ] as const,
  speech: [
    // For speech-to-text models
  ] as const,
  textToSpeech: [
    // For text-to-speech models
  ] as const,
};
```

2. **Update utility functions**:

```typescript
// src/constants/index.ts
export function getTextGenerationModels(): readonly string[] {
  return CLOUDFLARE_AI_MODELS.textGeneration;
}

export function getSpeechModels(): readonly string[] {
  return CLOUDFLARE_AI_MODELS.speech;
}

export function getTextToSpeechModels(): readonly string[] {
  return CLOUDFLARE_AI_MODELS.textToSpeech;
}
```

3. **Test the integration**:

```bash
pnpm dev
# Test the new model in the UI
```

## ü§ù **Contributing**

We welcome contributions!

### Quick Start for Contributors

```bash
# Fork the repo and clone your fork
git clone https://github.com/devhims/opengpt.git

# Create a feature branch
git checkout -b feature/new-feature

# Make your changes and test
pnpm dev

# Run linting and formatting
pnpm lint
pnpm format

# Commit using conventional commits
git commit -m "feat: add new feature"

# Push and create a PR
git push origin feature/new-feature
```

### Code Style

- **TypeScript**: Strict mode enabled
- **Formatting**: Prettier with Tailwind class sorting
- **Linting**: ESLint with Next.js rules

## üìÑ **License**

This project is licensed under the MIT License.

<div align="center">

**Made with ‚ù§Ô∏è for the AI community**

‚≠ê **Star this repo** if you find it useful!

</div>
